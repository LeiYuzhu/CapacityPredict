{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from utils.metrics import metric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import ConcatDataset\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RevIN类实现了可逆的实例归一化操作，可以在前向传播和反向传播过程中进行归一化和反归一化操作。\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels  输入特征的数量\n",
    "        :param eps: a value added for numerical stability  用于数值稳定性的值\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features  # 输入特征的数量\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, mode:str):  # 根据mode的值，选择执行归一化或反归一化操作\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):  # 计算输入x的均值和标准差。\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):  # 执行归一化操作，即减去均值，然后除以标准差。\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):  # 执行反归一化操作，即先乘以标准差，最后加上均值。\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intra_Patch_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Intra_Patch_MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inter_Patch_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Inter_Patch_MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, device, d_ff, num_nodes, patch_nums, patch_size, dynamic, factorized, layer_number):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_nodes = num_nodes\n",
    "        self.patch_nums = patch_nums\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_number = layer_number\n",
    "\n",
    "        # Intra and Inter Patch MLP\n",
    "        self.intra_patch_mlp = Intra_Patch_MLP(patch_size * num_nodes, d_ff)\n",
    "        self.intra_Linear = nn.Linear(self.patch_nums*self.patch_size, self.patch_nums*self.patch_size)\n",
    "        self.inter_patch_mlp = Inter_Patch_MLP(self.patch_nums, self.patch_nums)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # FeedForward layer\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(num_nodes, d_ff, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(d_ff, num_nodes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        intra_out_concat = None\n",
    "\n",
    "        # Intra Patch MLP\n",
    "        for i in range(self.patch_nums):\n",
    "            t = x[:, i * self.patch_size:(i + 1) * self.patch_size, :]  # x: [batch, seq_len, num_nodes], t: [batch, patch_size, num_nodes]\n",
    "            t = t.view(batch_size, -1)  # Flatten the input  t: [batch, patch_size * num_nodes]\n",
    "            t = self.intra_patch_mlp(t)\n",
    "            t = t.view(batch_size, self.patch_size, -1)  # Reshape the output  t: [batch, patch_size, num_nodes]\n",
    "\n",
    "            if intra_out_concat is None:\n",
    "                intra_out_concat = t\n",
    "            else:\n",
    "                intra_out_concat = torch.cat([intra_out_concat, t], dim=1)  # Concatenate the output  [batch, patch_size * patch_nums, num_nodes]\n",
    "\n",
    "        intra_out_concat = intra_out_concat.permute(0, 2, 1)  # [batch, patch_size * patch_nums, num_nodes] -> [batch, num_nodes, patch_size * patch_nums]\n",
    "        intra_out_concat = self.intra_Linear(intra_out_concat)  # [batch, num_nodes, patch_size * patch_nums]-> [batch, num_nodes, patch_size * patch_nums]\n",
    "        intra_out_concat = intra_out_concat.permute(0, 2, 1)  # [batch, num_nodes, patch_size * patch_nums] -> [batch, patch_size * patch_nums, num_nodes] \n",
    "\n",
    "        # Inter Patch MLP\n",
    "        u = x.unfold(dimension=1, size=self.patch_size, step=self.patch_size)   # 将张量沿指定维度展开成滑动窗口，创建一个包含多个小块（patches）的张量[batch, patch_num, patch_size, num_nodes]\n",
    "        u = u.permute(0, 3, 2, 1)  # [batch, num_nodes, patch_size, patch_num]\n",
    "        inter_out = self.inter_patch_mlp(u)\n",
    "        inter_out = inter_out.permute(0, 3, 2, 1)  # [batch, patch_num, patch_size, num_nodes]\n",
    "        inter_out = inter_out.reshape(batch_size, self.patch_nums * self.patch_size, self.num_nodes)  # [batch, patch_nums * patch_size, num_nodes]\n",
    "\n",
    "        out = inter_out + intra_out_concat  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchModel(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(PatchModel, self).__init__()\n",
    "        self.layer_nums = configs.layer_nums  # 设置pathway的层数\n",
    "        self.num_nodes = configs.num_nodes  # 输入特征的维度\n",
    "        self.pre_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len  # 20241210 configs.seq_len改为128，对应趋势感知嵌入扩维的改动\n",
    "        self.k = configs.k\n",
    "        self.num_experts_list = configs.num_experts_list\n",
    "        self.patch_size_list = configs.patch_size_list\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.revin = configs.revin  # 默认为1\n",
    "        if self.revin:\n",
    "            self.revin_layer = RevIN(num_features=configs.num_nodes)  # 进行了实例归一化操作，使样本的特征分布符合标准正态分布\n",
    "\n",
    "        self.AMS_lists = nn.ModuleList()  # 定义一个空的模块列表，多层自适应多尺度模块\n",
    "        self.device = torch.device('cuda:{}'.format(configs.gpu))  # 设置设备\n",
    "\n",
    "        for num in range(self.layer_nums):  # 依次添加多层自适应多尺度模块\n",
    "            self.AMS_lists.append(\n",
    "                AMS(self.seq_len, self.seq_len, self.num_experts_list[num], self.device, k=self.k,\n",
    "                    num_nodes=self.num_nodes, patch_size=self.patch_size_list[num], noisy_gating=True,\n",
    "                    d_ff=self.d_ff, layer_number=num + 1))  # 自适应多尺度模块\n",
    "        self.projections = nn.Sequential(nn.Linear(self.seq_len, self.pre_len) )  # 线性层，预测器\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.revin:\n",
    "            x = self.revin_layer(x, 'norm')  # 实例归一化 x:（batch_size, seq_len, num_nodes）\n",
    "        out = x  # x:（batch_size, seq_len, num_nodes）\n",
    "        batch_size = x.shape[0]\n",
    "        for layer in self.AMS_lists:\n",
    "            out = layer(out)   # 多层自适应多尺度模块  out:（batch_size, seq_len, num_nodes）\n",
    "        out = out.permute(0,2,1).reshape(batch_size, self.num_nodes, -1)  # 原来的第二个和第三个维度被交换了位置，然后将第三个维度展平  out:（batch_size, num_nodes, seq_len）\n",
    "        out = self.projections(out).transpose(2, 1)  # 通过线性层，并将第三个维度和第二个维度交换位置  out:（batch_size, pre_len, num_nodes）\n",
    "        if self.revin:\n",
    "            out = self.revin_layer(out, 'denorm')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_NASA(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None, data_path={'train':'data.csv'}):\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.pred_len = size[1]\n",
    "\n",
    "        assert flag in ['train1', 'train2', 'train3', 'test', 'val'] \n",
    "        self.root_path = root_path \n",
    "        self.data_path = data_path[flag]  \n",
    "        self.__read_data__()  # 读取数据\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler() # 标准化：将数据缩放到均值为0、标准差为1的范围内。\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))  # 读取数据\n",
    "\n",
    "        cols_data = df_raw.columns[1:]  \n",
    "        df_data = df_raw[cols_data]\n",
    "        data = df_data.values\n",
    "        self.data_x = data  # 根据train, test, val选择数据\n",
    "        self.data_y = data  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len \n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1  # 返回数据集的长度\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)  # 反标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'NASA': Dataset_NASA}\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]  # 根据数据集名称选择对应的数据集类class\n",
    "    \n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = False\n",
    "        batch_size = args.batch_size\n",
    "\n",
    "    else:  # 'train' or 'val'\n",
    "        shuffle_flag = True\n",
    "        drop_last = False\n",
    "        batch_size = args.batch_size\n",
    "\n",
    "    data_set = Data(root_path=args.root_path, data_path=args.data_path, flag=flag, size=[args.seq_len, args.pred_len])  # 实例化数据集类，得到数据集对象（初始化类）\n",
    "    print(flag, len(data_set))\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        data_set,  \n",
    "        batch_size=batch_size,  \n",
    "        shuffle=shuffle_flag,  # 是否在每个epoch开始的时候打乱数据。默认为False。\n",
    "        num_workers=args.num_workers,  # 用于数据加载的子进程数。默认为0，表示数据将在主进程中加载。\n",
    "        drop_last=drop_last)  # 如果数据集大小不能被批次大小整除，设置为True将丢弃最后一个不完整的批次。如果设为False并且数据集的大小不能被批次大小整除，则最后一个批次将更小。默认为False。\n",
    "        \n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "\n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "\n",
    "    def _acquire_device(self):\n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
    "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
    "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "            \n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Main(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Main, self).__init__(args)\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = PatchModel(self.args).float()\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.L1Loss()  # 平均绝对误差\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        self.model.eval()  # 模型评估\n",
    "        preds = []\n",
    "        trues = []\n",
    "        inputx = []\n",
    "        with torch.no_grad():  # with torch.no_grad():是一个上下文管理器，用于在其内部的代码块中禁用梯度计算。\n",
    "            for i, (batch_x, batch_y) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "                outputs = self.model(batch_x)\n",
    "                f_dim = -1 \n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "                # 将模型的输出outputs和目标值batch_y从GPU移动到CPU，并将它们从PyTorch张量转换为NumPy数组。\n",
    "\n",
    "                pred = outputs  \n",
    "                true = batch_y  \n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                inputx.append(batch_x.detach().cpu().numpy())\n",
    "\n",
    "        # 将预测结果、实际结果和输入数据转换为NumPy数组\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        inputx = np.concatenate(inputx, axis=0)\n",
    "\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])  # 将预测结果、实际结果和输入数据的形状转换为（batch_size*seq_len, pred_len, features）\n",
    "\n",
    "        mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)  # 计算预测结果的评价指标\n",
    "        self.model.train()\n",
    "        return mae   # 返回验证损失\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data1, train_loader1 = self._get_data(flag='train1')  # 获取数据处理器\n",
    "        train_data2, train_loader2 = self._get_data(flag='train2')  # 获取数据处理器\n",
    "        train_data3, train_loader3 = self._get_data(flag='train3')  # 获取数据处理器\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        # 合并数据集\n",
    "        train_data = ConcatDataset([train_data1, train_data2, train_data3])\n",
    "\n",
    "        # 创建一个新的数据加载器\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.args.batch_size, shuffle=True, num_workers=self.args.num_workers, drop_last=True)\n",
    "\n",
    "        path = os.path.join(self.args.checkpoints, setting) # 检查点路径\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # 创建路径\n",
    "\n",
    "        total_num = sum(p.numel() for p in self.model.parameters())  # 计算模型参数总数\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)  # 训练步数（batch的数量）\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True) # 早停\n",
    "        # verbose参数通常用于控制是否输出额外的调试信息。如果verbose=True，那么在早停条件满足时，EarlyStopping类会打印一些有关早停的信息，例如当前的epoch数、最佳的验证损失等。\n",
    "\n",
    "        model_optim = self._select_optimizer()  # 选择优化器\n",
    "        criterion = self._select_criterion()  # 选择损失函数\n",
    "\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=self.args.pct_start,\n",
    "                                            epochs=self.args.train_epochs,\n",
    "                                            max_lr=self.args.learning_rate)  # 学习率调整器\n",
    "        # OneCycleLR策略的思想是在训练过程中先线性地增加学习率，然后再线性地减少学习率。这种策略可以帮助模型快速收敛，并且可以防止模型陷入局部最优。\n",
    "        # pct_start参数用于控制增加学习率阶段的持续时间。它的值是一个介于0和1之间的浮点数，表示增加学习率阶段的epoch数占总epoch数的比例。\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for epoch in range(self.args.train_epochs):  # 训练epochs\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "            # train_loader是一个DataLoader对象，它可以将数据集分成多个批次，每个批次包含多个样本。在每次迭代中，train_loader都会返回一个批次的数据。\n",
    "            for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(self.device)  # （batch_size, seq_len, features）\n",
    "\n",
    "                batch_y = batch_y.float().to(self.device)  # （batch_size, pred_len, features）\n",
    "                # 无论batch_x原来的数据类型是什么，无论原来存储在哪个设备上，都可以确保batch_x是float32类型并且存储在正确的设备上，可以直接用于神经网络的计算。\n",
    "\n",
    "                outputs = self.model(batch_x)\n",
    "                f_dim = -1 \n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:  # 每100个batch打印一次\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())) # 打印迭代次数、epoch和损失值\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                loss.backward()  #计算梯度\n",
    "                model_optim.step()  # 更新参数\n",
    "\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, self.args, printout=False) # 调整学习率\n",
    "                # 如果printout=True，那么每次调整学习率时都会打印一些信息，例如当前的学习率等。如果printout=False，那么不会打印这些信息。\n",
    "                scheduler.step() # 更新学习率\n",
    "            # 每个epoch结束后，计算验证损失和测试损失\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            test_loss = self.vali(test_data, test_loader, criterion)\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f}  Test Loss: {3:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, test_loss))\n",
    "            # 收集损失数据\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            early_stopping(train_loss, self.model, path)  \n",
    "            # 检查vali_loss是否比之前看到的最佳验证损失更好。如果是，它就保存当前的模型到path；如果不是，并且在一定数量的连续训练轮中没有改善，它就将early_stop属性设置为True。\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "        # # 绘制损失曲线\n",
    "        # # 保存 true 和 pred 数据\n",
    "        # folder_path = './test_results/' + setting + '/'\n",
    "        # np.save(os.path.join(folder_path, 'train_losses.npy'), train_losses)\n",
    "        # np.save(os.path.join(folder_path, 'test_losses.npy'), test_losses)\n",
    "\n",
    "        # 加载最佳模型\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        return self.model  # 返回模型\n",
    "\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='test')  # 获取测试数据\n",
    "\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth'))) # 加载最佳模型\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        inputx = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path) # 创建测试结果路径\n",
    "        infer_time = []\n",
    "        self.model.eval()  # self.model.eval()是一个PyTorch中的方法，用于将模型设置为评估模式。\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "                # 记录开始时间\n",
    "                start_time = time.time()\n",
    "                outputs = self.model(batch_x)\n",
    "                # 记录结束时间\n",
    "                end_time = time.time()\n",
    "                # 计算推理用时\n",
    "                inference_time = end_time - start_time\n",
    "                infer_time.append(inference_time)\n",
    "                f_dim = -1 \n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]  # 取预测长度的数据\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "                # 将模型的输出outputs和目标值batch_y从GPU移动到CPU，并将它们从PyTorch张量转换为NumPy数组。\n",
    "\n",
    "                pred = outputs  \n",
    "                true = batch_y  \n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                inputx.append(batch_x.detach().cpu().numpy())\n",
    "        print('Inference time: ', np.mean(infer_time))\n",
    "        # 将预测结果、实际结果和输入数据转换为NumPy数组\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        inputx = np.concatenate(inputx, axis=0)\n",
    "\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])  # 将预测结果、实际结果和输入数据的形状转换为（batch_size*seq_len, pred_len, features）\n",
    "\n",
    "        mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)  # 计算预测结果的评价指标\n",
    "        print('rmse:{}, mae:{}, rse:{}'.format(rmse, mae, rse))\n",
    "        f = open(\"result.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('rmse:{}, mae:{}, rse:{}'.format(rmse, mae, rse)) \n",
    "        # f.write('\\n')\n",
    "        f.write('\\n')  # 将评价指标写入文件\n",
    "        f.close()\n",
    "        return preds, trues  # 返回预测结果、实际结果和输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(args):\n",
    "    Exp = Exp_Main\n",
    "    if args.is_training:  # 训练\n",
    "        for ii in range(args.itr):  # 实验次数\n",
    "            setting = '{}_{}_sl{}_pl{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                # args.features,\n",
    "                args.seq_len,\n",
    "                args.pred_len, ii)  #记录实验设置\n",
    "            # format函数用于格式化字符串。在这个例子中，每个{}都会被format函数的一个参数替换。参数的顺序与{}中的顺序相对应。\n",
    "\n",
    "            exp = Exp(args)  # set experiments\n",
    "\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)  # 训练\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)  # 测试\n",
    "            torch.cuda.empty_cache()  # 释放GPU缓存\n",
    "    else:\n",
    "        ii = 2\n",
    "        setting = '{}_{}_sl{}_pl{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            # args.features,\n",
    "            args.seq_len,\n",
    "            args.pred_len, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()  # 释放GPU缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : B0007_PathFormer_sl36_pl1_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train1 96\n",
      "train2 132\n",
      "train3 132\n",
      "test 132\n",
      "Epoch: 1 cost time: 0.0860135555267334\n",
      "Epoch: 1, Steps: 11 | Train Loss: 0.0697246  Test Loss: 0.0518708\n",
      "Validation loss decreased (inf --> 0.069725).  Saving model ...\n",
      "Updating learning rate to 0.00020743212569329043\n",
      "Epoch: 2 cost time: 0.08111405372619629\n",
      "Epoch: 2, Steps: 11 | Train Loss: 0.0675455  Test Loss: 0.0494756\n",
      "Validation loss decreased (0.069725 --> 0.067546).  Saving model ...\n",
      "Updating learning rate to 0.00022968247236289548\n",
      "Epoch: 3 cost time: 0.08494210243225098\n",
      "Epoch: 3, Steps: 11 | Train Loss: 0.0637382  Test Loss: 0.0460014\n",
      "Validation loss decreased (0.067546 --> 0.063738).  Saving model ...\n",
      "Updating learning rate to 0.0002666132338645091\n",
      "Epoch: 4 cost time: 0.0808420181274414\n",
      "Epoch: 4, Steps: 11 | Train Loss: 0.0579614  Test Loss: 0.0399620\n",
      "Validation loss decreased (0.063738 --> 0.057961).  Saving model ...\n",
      "Updating learning rate to 0.0003179956818136062\n",
      "Epoch: 5 cost time: 0.08003664016723633\n",
      "Epoch: 5, Steps: 11 | Train Loss: 0.0484503  Test Loss: 0.0284340\n",
      "Validation loss decreased (0.057961 --> 0.048450).  Saving model ...\n",
      "Updating learning rate to 0.0003835115822005313\n",
      "Epoch: 6 cost time: 0.0804893970489502\n",
      "Epoch: 6, Steps: 11 | Train Loss: 0.0311076  Test Loss: 0.0105264\n",
      "Validation loss decreased (0.048450 --> 0.031108).  Saving model ...\n",
      "Updating learning rate to 0.0004627551663531305\n",
      "Epoch: 7 cost time: 0.0792229175567627\n",
      "Epoch: 7, Steps: 11 | Train Loss: 0.0235098  Test Loss: 0.0146568\n",
      "Validation loss decreased (0.031108 --> 0.023510).  Saving model ...\n",
      "Updating learning rate to 0.0005552356440398963\n",
      "Epoch: 8 cost time: 0.07774710655212402\n",
      "Epoch: 8, Steps: 11 | Train Loss: 0.0194158  Test Loss: 0.0111699\n",
      "Validation loss decreased (0.023510 --> 0.019416).  Saving model ...\n",
      "Updating learning rate to 0.0006603802431488772\n",
      "Epoch: 9 cost time: 0.08328890800476074\n",
      "Epoch: 9, Steps: 11 | Train Loss: 0.0198181  Test Loss: 0.0106165\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0007775377571162803\n",
      "Epoch: 10 cost time: 0.0784912109375\n",
      "Epoch: 10, Steps: 11 | Train Loss: 0.0182877  Test Loss: 0.0099614\n",
      "Validation loss decreased (0.019416 --> 0.018288).  Saving model ...\n",
      "Updating learning rate to 0.0009059825781340073\n",
      "Epoch: 11 cost time: 0.07823896408081055\n",
      "Epoch: 11, Steps: 11 | Train Loss: 0.0174530  Test Loss: 0.0097788\n",
      "Validation loss decreased (0.018288 --> 0.017453).  Saving model ...\n",
      "Updating learning rate to 0.0010449191911566782\n",
      "Epoch: 12 cost time: 0.07999205589294434\n",
      "Epoch: 12, Steps: 11 | Train Loss: 0.0171728  Test Loss: 0.0090855\n",
      "Validation loss decreased (0.017453 --> 0.017173).  Saving model ...\n",
      "Updating learning rate to 0.001193487100874806\n",
      "Epoch: 13 cost time: 0.0792396068572998\n",
      "Epoch: 13, Steps: 11 | Train Loss: 0.0157338  Test Loss: 0.0083154\n",
      "Validation loss decreased (0.017173 --> 0.015734).  Saving model ...\n",
      "Updating learning rate to 0.0013507661611392116\n",
      "Epoch: 14 cost time: 0.0790867805480957\n",
      "Epoch: 14, Steps: 11 | Train Loss: 0.0143868  Test Loss: 0.0073161\n",
      "Validation loss decreased (0.015734 --> 0.014387).  Saving model ...\n",
      "Updating learning rate to 0.0015157822738292221\n",
      "Epoch: 15 cost time: 0.08594393730163574\n",
      "Epoch: 15, Steps: 11 | Train Loss: 0.0125969  Test Loss: 0.0069705\n",
      "Validation loss decreased (0.014387 --> 0.012597).  Saving model ...\n",
      "Updating learning rate to 0.0016875134218690617\n",
      "Epoch: 16 cost time: 0.08188271522521973\n",
      "Epoch: 16, Steps: 11 | Train Loss: 0.0116237  Test Loss: 0.0064252\n",
      "Validation loss decreased (0.012597 --> 0.011624).  Saving model ...\n",
      "Updating learning rate to 0.0018648959990273258\n",
      "Epoch: 17 cost time: 0.08245062828063965\n",
      "Epoch: 17, Steps: 11 | Train Loss: 0.0108691  Test Loss: 0.0066211\n",
      "Validation loss decreased (0.011624 --> 0.010869).  Saving model ...\n",
      "Updating learning rate to 0.0020468313972963155\n",
      "Epoch: 18 cost time: 0.08124732971191406\n",
      "Epoch: 18, Steps: 11 | Train Loss: 0.0108274  Test Loss: 0.0063750\n",
      "Validation loss decreased (0.010869 --> 0.010827).  Saving model ...\n",
      "Updating learning rate to 0.002232192811052702\n",
      "Epoch: 19 cost time: 0.07950186729431152\n",
      "Epoch: 19, Steps: 11 | Train Loss: 0.0106447  Test Loss: 0.0068904\n",
      "Validation loss decreased (0.010827 --> 0.010645).  Saving model ...\n",
      "Updating learning rate to 0.0024198322158583817\n",
      "Epoch: 20 cost time: 0.08027148246765137\n",
      "Epoch: 20, Steps: 11 | Train Loss: 0.0116721  Test Loss: 0.0097641\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0026085874786787467\n",
      "Epoch: 21 cost time: 0.08437681198120117\n",
      "Epoch: 21, Steps: 11 | Train Loss: 0.0102209  Test Loss: 0.0063420\n",
      "Validation loss decreased (0.010645 --> 0.010221).  Saving model ...\n",
      "Updating learning rate to 0.002797289555481671\n",
      "Epoch: 22 cost time: 0.07771682739257812\n",
      "Epoch: 22, Steps: 11 | Train Loss: 0.0103908  Test Loss: 0.0059656\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.002984769731639334\n",
      "Epoch: 23 cost time: 0.0784921646118164\n",
      "Epoch: 23, Steps: 11 | Train Loss: 0.0096153  Test Loss: 0.0087291\n",
      "Validation loss decreased (0.010221 --> 0.009615).  Saving model ...\n",
      "Updating learning rate to 0.0031698668602898996\n",
      "Epoch: 24 cost time: 0.07846999168395996\n",
      "Epoch: 24, Steps: 11 | Train Loss: 0.0096987  Test Loss: 0.0057809\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.003351434553828701\n",
      "Epoch: 25 cost time: 0.07859420776367188\n",
      "Epoch: 25, Steps: 11 | Train Loss: 0.0097036  Test Loss: 0.0054920\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0035283482839888903\n",
      "Epoch: 26 cost time: 0.07578015327453613\n",
      "Epoch: 26, Steps: 11 | Train Loss: 0.0085119  Test Loss: 0.0056411\n",
      "Validation loss decreased (0.009615 --> 0.008512).  Saving model ...\n",
      "Updating learning rate to 0.003699512346537615\n",
      "Epoch: 27 cost time: 0.07667279243469238\n",
      "Epoch: 27, Steps: 11 | Train Loss: 0.0090151  Test Loss: 0.0055298\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.003863866647452345\n",
      "Epoch: 28 cost time: 0.07242274284362793\n",
      "Epoch: 28, Steps: 11 | Train Loss: 0.0097859  Test Loss: 0.0058265\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004020393268547556\n",
      "Epoch: 29 cost time: 0.07346701622009277\n",
      "Epoch: 29, Steps: 11 | Train Loss: 0.0101234  Test Loss: 0.0104660\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004168122771887976\n",
      "Epoch: 30 cost time: 0.0758814811706543\n",
      "Epoch: 30, Steps: 11 | Train Loss: 0.0112098  Test Loss: 0.0106879\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004306140203942409\n",
      "Epoch: 31 cost time: 0.07722663879394531\n",
      "Epoch: 31, Steps: 11 | Train Loss: 0.0099257  Test Loss: 0.0056134\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004433590762291778\n",
      "Epoch: 32 cost time: 0.07494473457336426\n",
      "Epoch: 32, Steps: 11 | Train Loss: 0.0082522  Test Loss: 0.0054122\n",
      "Validation loss decreased (0.008512 --> 0.008252).  Saving model ...\n",
      "Updating learning rate to 0.004549685089794972\n",
      "Epoch: 33 cost time: 0.07405352592468262\n",
      "Epoch: 33, Steps: 11 | Train Loss: 0.0088517  Test Loss: 0.0052789\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004653704163423424\n",
      "Epoch: 34 cost time: 0.07605290412902832\n",
      "Epoch: 34, Steps: 11 | Train Loss: 0.0088341  Test Loss: 0.0066909\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004745003747485712\n",
      "Epoch: 35 cost time: 0.07336258888244629\n",
      "Epoch: 35, Steps: 11 | Train Loss: 0.0088087  Test Loss: 0.0056962\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0048230183836614204\n",
      "Epoch: 36 cost time: 0.09072375297546387\n",
      "Epoch: 36, Steps: 11 | Train Loss: 0.0083262  Test Loss: 0.0062959\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004887264893132239\n",
      "Epoch: 37 cost time: 0.07655739784240723\n",
      "Epoch: 37, Steps: 11 | Train Loss: 0.0102165  Test Loss: 0.0063383\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004937345369120049\n",
      "Epoch: 38 cost time: 0.07256722450256348\n",
      "Epoch: 38, Steps: 11 | Train Loss: 0.0090154  Test Loss: 0.0081251\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.004972949641297917\n",
      "Epoch: 39 cost time: 0.07406020164489746\n",
      "Epoch: 39, Steps: 11 | Train Loss: 0.0091393  Test Loss: 0.0055528\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.004993857196810804\n",
      "Epoch: 40 cost time: 0.07459521293640137\n",
      "Epoch: 40, Steps: 11 | Train Loss: 0.0080098  Test Loss: 0.0056793\n",
      "Validation loss decreased (0.008252 --> 0.008010).  Saving model ...\n",
      "Updating learning rate to 0.00499997167829922\n",
      "Epoch: 41 cost time: 0.0719764232635498\n",
      "Epoch: 41, Steps: 11 | Train Loss: 0.0093279  Test Loss: 0.0076388\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0049959227761243\n",
      "Epoch: 42 cost time: 0.07478046417236328\n",
      "Epoch: 42, Steps: 11 | Train Loss: 0.0090310  Test Loss: 0.0053241\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004985032750518793\n",
      "Epoch: 43 cost time: 0.07327437400817871\n",
      "Epoch: 43, Steps: 11 | Train Loss: 0.0079536  Test Loss: 0.0058867\n",
      "Validation loss decreased (0.008010 --> 0.007954).  Saving model ...\n",
      "Updating learning rate to 0.004967331450285928\n",
      "Epoch: 44 cost time: 0.07279777526855469\n",
      "Epoch: 44, Steps: 11 | Train Loss: 0.0077916  Test Loss: 0.0055164\n",
      "Validation loss decreased (0.007954 --> 0.007792).  Saving model ...\n",
      "Updating learning rate to 0.004942867393459239\n",
      "Epoch: 45 cost time: 0.07519817352294922\n",
      "Epoch: 45, Steps: 11 | Train Loss: 0.0076007  Test Loss: 0.0052174\n",
      "Validation loss decreased (0.007792 --> 0.007601).  Saving model ...\n",
      "Updating learning rate to 0.004911707634318015\n",
      "Epoch: 46 cost time: 0.07358860969543457\n",
      "Epoch: 46, Steps: 11 | Train Loss: 0.0078127  Test Loss: 0.0082734\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004873937579596172\n",
      "Epoch: 47 cost time: 0.07095503807067871\n",
      "Epoch: 47, Steps: 11 | Train Loss: 0.0087469  Test Loss: 0.0062261\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004829660754388339\n",
      "Epoch: 48 cost time: 0.07802081108093262\n",
      "Epoch: 48, Steps: 11 | Train Loss: 0.0080181  Test Loss: 0.0054251\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004778998518394766\n",
      "Epoch: 49 cost time: 0.0750586986541748\n",
      "Epoch: 49, Steps: 11 | Train Loss: 0.0077017  Test Loss: 0.0057764\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004722089733282822\n",
      "Epoch: 50 cost time: 0.07369494438171387\n",
      "Epoch: 50, Steps: 11 | Train Loss: 0.0081404  Test Loss: 0.0087002\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004659090382076819\n",
      "Epoch: 51 cost time: 0.07692098617553711\n",
      "Epoch: 51, Steps: 11 | Train Loss: 0.0092208  Test Loss: 0.0049693\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.004590173141619381\n",
      "Epoch: 52 cost time: 0.0726008415222168\n",
      "Epoch: 52, Steps: 11 | Train Loss: 0.0105877  Test Loss: 0.0065764\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.004515526909276222\n",
      "Epoch: 53 cost time: 0.07056760787963867\n",
      "Epoch: 53, Steps: 11 | Train Loss: 0.0106501  Test Loss: 0.0079755\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 0.0044353562851816\n",
      "Epoch: 54 cost time: 0.07321858406066895\n",
      "Epoch: 54, Steps: 11 | Train Loss: 0.0109825  Test Loss: 0.0066531\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 0.004349881011443566\n",
      "Epoch: 55 cost time: 0.07529497146606445\n",
      "Epoch: 55, Steps: 11 | Train Loss: 0.0084266  Test Loss: 0.0051425\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 0.004259335369846122\n",
      "Epoch: 56 cost time: 0.07019543647766113\n",
      "Epoch: 56, Steps: 11 | Train Loss: 0.0081779  Test Loss: 0.0055146\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 0.004163967539699138\n",
      "Epoch: 57 cost time: 0.07572817802429199\n",
      "Epoch: 57, Steps: 11 | Train Loss: 0.0087403  Test Loss: 0.0070756\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 0.004064038917596108\n",
      "Epoch: 58 cost time: 0.07350873947143555\n",
      "Epoch: 58, Steps: 11 | Train Loss: 0.0082174  Test Loss: 0.0048999\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 0.003959823400944265\n",
      "Epoch: 59 cost time: 0.07186031341552734\n",
      "Epoch: 59, Steps: 11 | Train Loss: 0.0070424  Test Loss: 0.0051519\n",
      "Validation loss decreased (0.007601 --> 0.007042).  Saving model ...\n",
      "Updating learning rate to 0.00385160663723082\n",
      "Epoch: 60 cost time: 0.07813453674316406\n",
      "Epoch: 60, Steps: 11 | Train Loss: 0.0069672  Test Loss: 0.0054714\n",
      "Validation loss decreased (0.007042 --> 0.006967).  Saving model ...\n",
      "Updating learning rate to 0.003739685241083054\n",
      "Epoch: 61 cost time: 0.07356834411621094\n",
      "Epoch: 61, Steps: 11 | Train Loss: 0.0066957  Test Loss: 0.0048270\n",
      "Validation loss decreased (0.006967 --> 0.006696).  Saving model ...\n",
      "Updating learning rate to 0.00362436598126825\n",
      "Epoch: 62 cost time: 0.07373595237731934\n",
      "Epoch: 62, Steps: 11 | Train Loss: 0.0062397  Test Loss: 0.0045984\n",
      "Validation loss decreased (0.006696 --> 0.006240).  Saving model ...\n",
      "Updating learning rate to 0.0035059649398618154\n",
      "Epoch: 63 cost time: 0.07905888557434082\n",
      "Epoch: 63, Steps: 11 | Train Loss: 0.0065739  Test Loss: 0.0054707\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0033848066458882892\n",
      "Epoch: 64 cost time: 0.07399559020996094\n",
      "Epoch: 64, Steps: 11 | Train Loss: 0.0077399  Test Loss: 0.0048545\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.003261223185809845\n",
      "Epoch: 65 cost time: 0.07336997985839844\n",
      "Epoch: 65, Steps: 11 | Train Loss: 0.0075844  Test Loss: 0.0056018\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.003135553293300376\n",
      "Epoch: 66 cost time: 0.0829000473022461\n",
      "Epoch: 66, Steps: 11 | Train Loss: 0.0074111  Test Loss: 0.0048471\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.003008141420800042\n",
      "Epoch: 67 cost time: 0.07256460189819336\n",
      "Epoch: 67, Steps: 11 | Train Loss: 0.0068995  Test Loss: 0.0047677\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.0028793367953950785\n",
      "Epoch: 68 cost time: 0.07182669639587402\n",
      "Epoch: 68, Steps: 11 | Train Loss: 0.0067437  Test Loss: 0.0047305\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.0027494924616106187\n",
      "Epoch: 69 cost time: 0.0742185115814209\n",
      "Epoch: 69, Steps: 11 | Train Loss: 0.0059534  Test Loss: 0.0046056\n",
      "Validation loss decreased (0.006240 --> 0.005953).  Saving model ...\n",
      "Updating learning rate to 0.002618964313740198\n",
      "Epoch: 70 cost time: 0.07294440269470215\n",
      "Epoch: 70, Steps: 11 | Train Loss: 0.0060250  Test Loss: 0.0046043\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.002488110120364228\n",
      "Epoch: 71 cost time: 0.07293272018432617\n",
      "Epoch: 71, Steps: 11 | Train Loss: 0.0060389  Test Loss: 0.0045860\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0023572885437311885\n",
      "Epoch: 72 cost time: 0.07433843612670898\n",
      "Epoch: 72, Steps: 11 | Train Loss: 0.0057768  Test Loss: 0.0046366\n",
      "Validation loss decreased (0.005953 --> 0.005777).  Saving model ...\n",
      "Updating learning rate to 0.002226858156689337\n",
      "Epoch: 73 cost time: 0.07366108894348145\n",
      "Epoch: 73, Steps: 11 | Train Loss: 0.0056516  Test Loss: 0.0045501\n",
      "Validation loss decreased (0.005777 --> 0.005652).  Saving model ...\n",
      "Updating learning rate to 0.002097176459863446\n",
      "Epoch: 74 cost time: 0.07243537902832031\n",
      "Epoch: 74, Steps: 11 | Train Loss: 0.0056054  Test Loss: 0.0045173\n",
      "Validation loss decreased (0.005652 --> 0.005605).  Saving model ...\n",
      "Updating learning rate to 0.0019685989017704537\n",
      "Epoch: 75 cost time: 0.07717299461364746\n",
      "Epoch: 75, Steps: 11 | Train Loss: 0.0057468  Test Loss: 0.0044805\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.001841477904559777\n",
      "Epoch: 76 cost time: 0.07214045524597168\n",
      "Epoch: 76, Steps: 11 | Train Loss: 0.0053312  Test Loss: 0.0044679\n",
      "Validation loss decreased (0.005605 --> 0.005331).  Saving model ...\n",
      "Updating learning rate to 0.0017161618980486967\n",
      "Epoch: 77 cost time: 0.0810096263885498\n",
      "Epoch: 77, Steps: 11 | Train Loss: 0.0052643  Test Loss: 0.0044815\n",
      "Validation loss decreased (0.005331 --> 0.005264).  Saving model ...\n",
      "Updating learning rate to 0.0015929943647004514\n",
      "Epoch: 78 cost time: 0.07604455947875977\n",
      "Epoch: 78, Steps: 11 | Train Loss: 0.0052218  Test Loss: 0.0043944\n",
      "Validation loss decreased (0.005264 --> 0.005222).  Saving model ...\n",
      "Updating learning rate to 0.0014723128981626776\n",
      "Epoch: 79 cost time: 0.0744631290435791\n",
      "Epoch: 79, Steps: 11 | Train Loss: 0.0051661  Test Loss: 0.0044330\n",
      "Validation loss decreased (0.005222 --> 0.005166).  Saving model ...\n",
      "Updating learning rate to 0.0013544482779466913\n",
      "Epoch: 80 cost time: 0.08045125007629395\n",
      "Epoch: 80, Steps: 11 | Train Loss: 0.0048385  Test Loss: 0.0044411\n",
      "Validation loss decreased (0.005166 --> 0.004839).  Saving model ...\n",
      "Updating learning rate to 0.0012397235627838344\n",
      "Epoch: 81 cost time: 0.07763051986694336\n",
      "Epoch: 81, Steps: 11 | Train Loss: 0.0048807  Test Loss: 0.0044334\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0011284532051439504\n",
      "Epoch: 82 cost time: 0.07478189468383789\n",
      "Epoch: 82, Steps: 11 | Train Loss: 0.0048378  Test Loss: 0.0043836\n",
      "Validation loss decreased (0.004839 --> 0.004838).  Saving model ...\n",
      "Updating learning rate to 0.001020942189343022\n",
      "Epoch: 83 cost time: 0.07444524765014648\n",
      "Epoch: 83, Steps: 11 | Train Loss: 0.0051083  Test Loss: 0.0044330\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0009174851956023624\n",
      "Epoch: 84 cost time: 0.08200502395629883\n",
      "Epoch: 84, Steps: 11 | Train Loss: 0.0048913  Test Loss: 0.0043607\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0008183657923506057\n",
      "Epoch: 85 cost time: 0.07532238960266113\n",
      "Epoch: 85, Steps: 11 | Train Loss: 0.0049492  Test Loss: 0.0043669\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.000723855658982361\n",
      "Epoch: 86 cost time: 0.07719612121582031\n",
      "Epoch: 86, Steps: 11 | Train Loss: 0.0048901  Test Loss: 0.0043601\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0006342138412038708\n",
      "Epoch: 87 cost time: 0.07763171195983887\n",
      "Epoch: 87, Steps: 11 | Train Loss: 0.0045427  Test Loss: 0.0043451\n",
      "Validation loss decreased (0.004838 --> 0.004543).  Saving model ...\n",
      "Updating learning rate to 0.0005496860410067403\n",
      "Epoch: 88 cost time: 0.07704901695251465\n",
      "Epoch: 88, Steps: 11 | Train Loss: 0.0046312  Test Loss: 0.0043296\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00047050394321585307\n",
      "Epoch: 89 cost time: 0.08029961585998535\n",
      "Epoch: 89, Steps: 11 | Train Loss: 0.0045312  Test Loss: 0.0043362\n",
      "Validation loss decreased (0.004543 --> 0.004531).  Saving model ...\n",
      "Updating learning rate to 0.00039688458045737505\n",
      "Epoch: 90 cost time: 0.07904338836669922\n",
      "Epoch: 90, Steps: 11 | Train Loss: 0.0045956  Test Loss: 0.0044226\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0003290297382874084\n",
      "Epoch: 91 cost time: 0.08033204078674316\n",
      "Epoch: 91, Steps: 11 | Train Loss: 0.0045364  Test Loss: 0.0043461\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0002671254021118083\n",
      "Epoch: 92 cost time: 0.07881331443786621\n",
      "Epoch: 92, Steps: 11 | Train Loss: 0.0046069  Test Loss: 0.0043432\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0002113412474131142\n",
      "Epoch: 93 cost time: 0.0833120346069336\n",
      "Epoch: 93, Steps: 11 | Train Loss: 0.0045073  Test Loss: 0.0043705\n",
      "Validation loss decreased (0.004531 --> 0.004507).  Saving model ...\n",
      "Updating learning rate to 0.00016183017468184583\n",
      "Epoch: 94 cost time: 0.07559990882873535\n",
      "Epoch: 94, Steps: 11 | Train Loss: 0.0044231  Test Loss: 0.0043476\n",
      "Validation loss decreased (0.004507 --> 0.004423).  Saving model ...\n",
      "Updating learning rate to 0.00011872789032688815\n",
      "Epoch: 95 cost time: 0.0776064395904541\n",
      "Epoch: 95, Steps: 11 | Train Loss: 0.0044230  Test Loss: 0.0043579\n",
      "Validation loss decreased (0.004423 --> 0.004423).  Saving model ...\n",
      "Updating learning rate to 8.21525347136544e-05\n",
      "Epoch: 96 cost time: 0.07848310470581055\n",
      "Epoch: 96, Steps: 11 | Train Loss: 0.0038802  Test Loss: 0.0043849\n",
      "Validation loss decreased (0.004423 --> 0.003880).  Saving model ...\n",
      "Updating learning rate to 5.220435834955847e-05\n",
      "Epoch: 97 cost time: 0.07611942291259766\n",
      "Epoch: 97, Steps: 11 | Train Loss: 0.0044036  Test Loss: 0.0043735\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 2.89654471043424e-05\n",
      "Epoch: 98 cost time: 0.08539652824401855\n",
      "Epoch: 98, Steps: 11 | Train Loss: 0.0043391  Test Loss: 0.0043631\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 1.2499497218412443e-05\n",
      "Epoch: 99 cost time: 0.07932639122009277\n",
      "Epoch: 99, Steps: 11 | Train Loss: 0.0043829  Test Loss: 0.0043629\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 2.851640715871923e-06\n",
      "Epoch: 100 cost time: 0.07448458671569824\n",
      "Epoch: 100, Steps: 11 | Train Loss: 0.0043456  Test Loss: 0.0043628\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 4.8321700779777556e-08\n",
      ">>>>>>>testing : B0007_PathFormer_sl36_pl1_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 132\n",
      "Inference time:  0.0020006179809570314\n",
      "rmse:0.010809029452502728, mae:0.0043849097564816475, rse:0.08609423041343689\n",
      "Use CPU\n",
      ">>>>>>>start training : B0007_PathFormer_sl36_pl1_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train1 96\n",
      "train2 132\n",
      "train3 132\n",
      "test 132\n",
      "Epoch: 1 cost time: 0.07788729667663574\n",
      "Epoch: 1, Steps: 11 | Train Loss: 0.0716119  Test Loss: 0.0535696\n",
      "Validation loss decreased (inf --> 0.071612).  Saving model ...\n",
      "Updating learning rate to 0.00020743212569329043\n",
      "Epoch: 2 cost time: 0.07809996604919434\n",
      "Epoch: 2, Steps: 11 | Train Loss: 0.0684583  Test Loss: 0.0511745\n",
      "Validation loss decreased (0.071612 --> 0.068458).  Saving model ...\n",
      "Updating learning rate to 0.00022968247236289548\n",
      "Epoch: 3 cost time: 0.07584214210510254\n",
      "Epoch: 3, Steps: 11 | Train Loss: 0.0652795  Test Loss: 0.0481876\n",
      "Validation loss decreased (0.068458 --> 0.065280).  Saving model ...\n",
      "Updating learning rate to 0.0002666132338645091\n",
      "Epoch: 4 cost time: 0.07953143119812012\n",
      "Epoch: 4, Steps: 11 | Train Loss: 0.0609248  Test Loss: 0.0436900\n",
      "Validation loss decreased (0.065280 --> 0.060925).  Saving model ...\n",
      "Updating learning rate to 0.0003179956818136062\n",
      "Epoch: 5 cost time: 0.07971572875976562\n",
      "Epoch: 5, Steps: 11 | Train Loss: 0.0540954  Test Loss: 0.0359169\n",
      "Validation loss decreased (0.060925 --> 0.054095).  Saving model ...\n",
      "Updating learning rate to 0.0003835115822005313\n",
      "Epoch: 6 cost time: 0.08195614814758301\n",
      "Epoch: 6, Steps: 11 | Train Loss: 0.0406554  Test Loss: 0.0213048\n",
      "Validation loss decreased (0.054095 --> 0.040655).  Saving model ...\n",
      "Updating learning rate to 0.0004627551663531305\n",
      "Epoch: 7 cost time: 0.08397102355957031\n",
      "Epoch: 7, Steps: 11 | Train Loss: 0.0244890  Test Loss: 0.0141342\n",
      "Validation loss decreased (0.040655 --> 0.024489).  Saving model ...\n",
      "Updating learning rate to 0.0005552356440398963\n",
      "Epoch: 8 cost time: 0.0822441577911377\n",
      "Epoch: 8, Steps: 11 | Train Loss: 0.0221184  Test Loss: 0.0106111\n",
      "Validation loss decreased (0.024489 --> 0.022118).  Saving model ...\n",
      "Updating learning rate to 0.0006603802431488772\n",
      "Epoch: 9 cost time: 0.08422160148620605\n",
      "Epoch: 9, Steps: 11 | Train Loss: 0.0196465  Test Loss: 0.0127599\n",
      "Validation loss decreased (0.022118 --> 0.019646).  Saving model ...\n",
      "Updating learning rate to 0.0007775377571162803\n",
      "Epoch: 10 cost time: 0.08001065254211426\n",
      "Epoch: 10, Steps: 11 | Train Loss: 0.0191083  Test Loss: 0.0101331\n",
      "Validation loss decreased (0.019646 --> 0.019108).  Saving model ...\n",
      "Updating learning rate to 0.0009059825781340073\n",
      "Epoch: 11 cost time: 0.08908939361572266\n",
      "Epoch: 11, Steps: 11 | Train Loss: 0.0183002  Test Loss: 0.0102808\n",
      "Validation loss decreased (0.019108 --> 0.018300).  Saving model ...\n",
      "Updating learning rate to 0.0010449191911566782\n",
      "Epoch: 12 cost time: 0.08149576187133789\n",
      "Epoch: 12, Steps: 11 | Train Loss: 0.0171549  Test Loss: 0.0097854\n",
      "Validation loss decreased (0.018300 --> 0.017155).  Saving model ...\n",
      "Updating learning rate to 0.001193487100874806\n",
      "Epoch: 13 cost time: 0.08182287216186523\n",
      "Epoch: 13, Steps: 11 | Train Loss: 0.0166030  Test Loss: 0.0091689\n",
      "Validation loss decreased (0.017155 --> 0.016603).  Saving model ...\n",
      "Updating learning rate to 0.0013507661611392116\n",
      "Epoch: 14 cost time: 0.07921767234802246\n",
      "Epoch: 14, Steps: 11 | Train Loss: 0.0149296  Test Loss: 0.0077041\n",
      "Validation loss decreased (0.016603 --> 0.014930).  Saving model ...\n",
      "Updating learning rate to 0.0015157822738292221\n",
      "Epoch: 15 cost time: 0.07976603507995605\n",
      "Epoch: 15, Steps: 11 | Train Loss: 0.0138175  Test Loss: 0.0070339\n",
      "Validation loss decreased (0.014930 --> 0.013818).  Saving model ...\n",
      "Updating learning rate to 0.0016875134218690617\n",
      "Epoch: 16 cost time: 0.0813453197479248\n",
      "Epoch: 16, Steps: 11 | Train Loss: 0.0126658  Test Loss: 0.0067615\n",
      "Validation loss decreased (0.013818 --> 0.012666).  Saving model ...\n",
      "Updating learning rate to 0.0018648959990273258\n",
      "Epoch: 17 cost time: 0.0783393383026123\n",
      "Epoch: 17, Steps: 11 | Train Loss: 0.0114615  Test Loss: 0.0070757\n",
      "Validation loss decreased (0.012666 --> 0.011462).  Saving model ...\n",
      "Updating learning rate to 0.0020468313972963155\n",
      "Epoch: 18 cost time: 0.0790567398071289\n",
      "Epoch: 18, Steps: 11 | Train Loss: 0.0112419  Test Loss: 0.0076566\n",
      "Validation loss decreased (0.011462 --> 0.011242).  Saving model ...\n",
      "Updating learning rate to 0.002232192811052702\n",
      "Epoch: 19 cost time: 0.07962441444396973\n",
      "Epoch: 19, Steps: 11 | Train Loss: 0.0108125  Test Loss: 0.0063810\n",
      "Validation loss decreased (0.011242 --> 0.010813).  Saving model ...\n",
      "Updating learning rate to 0.0024198322158583817\n",
      "Epoch: 20 cost time: 0.07875561714172363\n",
      "Epoch: 20, Steps: 11 | Train Loss: 0.0098351  Test Loss: 0.0061644\n",
      "Validation loss decreased (0.010813 --> 0.009835).  Saving model ...\n",
      "Updating learning rate to 0.0026085874786787467\n",
      "Epoch: 21 cost time: 0.07824015617370605\n",
      "Epoch: 21, Steps: 11 | Train Loss: 0.0098781  Test Loss: 0.0060571\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.002797289555481671\n",
      "Epoch: 22 cost time: 0.08332204818725586\n",
      "Epoch: 22, Steps: 11 | Train Loss: 0.0095335  Test Loss: 0.0063247\n",
      "Validation loss decreased (0.009835 --> 0.009534).  Saving model ...\n",
      "Updating learning rate to 0.002984769731639334\n",
      "Epoch: 23 cost time: 0.0771329402923584\n",
      "Epoch: 23, Steps: 11 | Train Loss: 0.0095272  Test Loss: 0.0061345\n",
      "Validation loss decreased (0.009534 --> 0.009527).  Saving model ...\n",
      "Updating learning rate to 0.0031698668602898996\n",
      "Epoch: 24 cost time: 0.07609677314758301\n",
      "Epoch: 24, Steps: 11 | Train Loss: 0.0088109  Test Loss: 0.0058841\n",
      "Validation loss decreased (0.009527 --> 0.008811).  Saving model ...\n",
      "Updating learning rate to 0.003351434553828701\n",
      "Epoch: 25 cost time: 0.08090782165527344\n",
      "Epoch: 25, Steps: 11 | Train Loss: 0.0085007  Test Loss: 0.0057580\n",
      "Validation loss decreased (0.008811 --> 0.008501).  Saving model ...\n",
      "Updating learning rate to 0.0035283482839888903\n",
      "Epoch: 26 cost time: 0.07602453231811523\n",
      "Epoch: 26, Steps: 11 | Train Loss: 0.0096641  Test Loss: 0.0071376\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.003699512346537615\n",
      "Epoch: 27 cost time: 0.08018231391906738\n",
      "Epoch: 27, Steps: 11 | Train Loss: 0.0117794  Test Loss: 0.0059896\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.003863866647452345\n",
      "Epoch: 28 cost time: 0.07954764366149902\n",
      "Epoch: 28, Steps: 11 | Train Loss: 0.0103382  Test Loss: 0.0067527\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004020393268547556\n",
      "Epoch: 29 cost time: 0.07697272300720215\n",
      "Epoch: 29, Steps: 11 | Train Loss: 0.0092496  Test Loss: 0.0072880\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004168122771887976\n",
      "Epoch: 30 cost time: 0.0782473087310791\n",
      "Epoch: 30, Steps: 11 | Train Loss: 0.0106337  Test Loss: 0.0061130\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004306140203942409\n",
      "Epoch: 31 cost time: 0.07790851593017578\n",
      "Epoch: 31, Steps: 11 | Train Loss: 0.0096970  Test Loss: 0.0056959\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.004433590762291778\n",
      "Epoch: 32 cost time: 0.07866883277893066\n",
      "Epoch: 32, Steps: 11 | Train Loss: 0.0087944  Test Loss: 0.0071981\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.004549685089794972\n",
      "Epoch: 33 cost time: 0.07657885551452637\n",
      "Epoch: 33, Steps: 11 | Train Loss: 0.0099838  Test Loss: 0.0068789\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 0.004653704163423424\n",
      "Epoch: 34 cost time: 0.0787363052368164\n",
      "Epoch: 34, Steps: 11 | Train Loss: 0.0095825  Test Loss: 0.0078903\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 0.004745003747485712\n",
      "Epoch: 35 cost time: 0.07649803161621094\n",
      "Epoch: 35, Steps: 11 | Train Loss: 0.0112460  Test Loss: 0.0073333\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 0.0048230183836614204\n",
      "Epoch: 36 cost time: 0.07658863067626953\n",
      "Epoch: 36, Steps: 11 | Train Loss: 0.0092390  Test Loss: 0.0056389\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 0.004887264893132239\n",
      "Epoch: 37 cost time: 0.08150935173034668\n",
      "Epoch: 37, Steps: 11 | Train Loss: 0.0088809  Test Loss: 0.0060285\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 0.004937345369120049\n",
      "Epoch: 38 cost time: 0.07903456687927246\n",
      "Epoch: 38, Steps: 11 | Train Loss: 0.0091482  Test Loss: 0.0059350\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 0.004972949641297917\n",
      "Epoch: 39 cost time: 0.07724809646606445\n",
      "Epoch: 39, Steps: 11 | Train Loss: 0.0084907  Test Loss: 0.0055606\n",
      "Validation loss decreased (0.008501 --> 0.008491).  Saving model ...\n",
      "Updating learning rate to 0.004993857196810804\n",
      "Epoch: 40 cost time: 0.0790092945098877\n",
      "Epoch: 40, Steps: 11 | Train Loss: 0.0083464  Test Loss: 0.0057477\n",
      "Validation loss decreased (0.008491 --> 0.008346).  Saving model ...\n",
      "Updating learning rate to 0.00499997167829922\n",
      "Epoch: 41 cost time: 0.07927370071411133\n",
      "Epoch: 41, Steps: 11 | Train Loss: 0.0090617  Test Loss: 0.0089327\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0049959227761243\n",
      "Epoch: 42 cost time: 0.07653522491455078\n",
      "Epoch: 42, Steps: 11 | Train Loss: 0.0115789  Test Loss: 0.0081107\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004985032750518793\n",
      "Epoch: 43 cost time: 0.07616996765136719\n",
      "Epoch: 43, Steps: 11 | Train Loss: 0.0101488  Test Loss: 0.0057661\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004967331450285928\n",
      "Epoch: 44 cost time: 0.07557916641235352\n",
      "Epoch: 44, Steps: 11 | Train Loss: 0.0104786  Test Loss: 0.0064797\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004942867393459239\n",
      "Epoch: 45 cost time: 0.0757744312286377\n",
      "Epoch: 45, Steps: 11 | Train Loss: 0.0093028  Test Loss: 0.0059481\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004911707634318015\n",
      "Epoch: 46 cost time: 0.07816958427429199\n",
      "Epoch: 46, Steps: 11 | Train Loss: 0.0084003  Test Loss: 0.0052303\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.004873937579596172\n",
      "Epoch: 47 cost time: 0.07510781288146973\n",
      "Epoch: 47, Steps: 11 | Train Loss: 0.0085616  Test Loss: 0.0052014\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.004829660754388339\n",
      "Epoch: 48 cost time: 0.07641100883483887\n",
      "Epoch: 48, Steps: 11 | Train Loss: 0.0084987  Test Loss: 0.0062180\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 0.004778998518394766\n",
      "Epoch: 49 cost time: 0.07697892189025879\n",
      "Epoch: 49, Steps: 11 | Train Loss: 0.0081709  Test Loss: 0.0049871\n",
      "Validation loss decreased (0.008346 --> 0.008171).  Saving model ...\n",
      "Updating learning rate to 0.004722089733282822\n",
      "Epoch: 50 cost time: 0.07790780067443848\n",
      "Epoch: 50, Steps: 11 | Train Loss: 0.0077830  Test Loss: 0.0071400\n",
      "Validation loss decreased (0.008171 --> 0.007783).  Saving model ...\n",
      "Updating learning rate to 0.004659090382076819\n",
      "Epoch: 51 cost time: 0.0742180347442627\n",
      "Epoch: 51, Steps: 11 | Train Loss: 0.0083434  Test Loss: 0.0053195\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004590173141619381\n",
      "Epoch: 52 cost time: 0.08037614822387695\n",
      "Epoch: 52, Steps: 11 | Train Loss: 0.0073590  Test Loss: 0.0048103\n",
      "Validation loss decreased (0.007783 --> 0.007359).  Saving model ...\n",
      "Updating learning rate to 0.004515526909276222\n",
      "Epoch: 53 cost time: 0.07794761657714844\n",
      "Epoch: 53, Steps: 11 | Train Loss: 0.0076748  Test Loss: 0.0059443\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0044353562851816\n",
      "Epoch: 54 cost time: 0.07444119453430176\n",
      "Epoch: 54, Steps: 11 | Train Loss: 0.0080468  Test Loss: 0.0050918\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004349881011443566\n",
      "Epoch: 55 cost time: 0.07369613647460938\n",
      "Epoch: 55, Steps: 11 | Train Loss: 0.0084728  Test Loss: 0.0072336\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004259335369846122\n",
      "Epoch: 56 cost time: 0.06968212127685547\n",
      "Epoch: 56, Steps: 11 | Train Loss: 0.0083377  Test Loss: 0.0053055\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004163967539699138\n",
      "Epoch: 57 cost time: 0.06910371780395508\n",
      "Epoch: 57, Steps: 11 | Train Loss: 0.0075205  Test Loss: 0.0062927\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004064038917596108\n",
      "Epoch: 58 cost time: 0.06905651092529297\n",
      "Epoch: 58, Steps: 11 | Train Loss: 0.0079265  Test Loss: 0.0057065\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.003959823400944265\n",
      "Epoch: 59 cost time: 0.07190322875976562\n",
      "Epoch: 59, Steps: 11 | Train Loss: 0.0068358  Test Loss: 0.0050286\n",
      "Validation loss decreased (0.007359 --> 0.006836).  Saving model ...\n",
      "Updating learning rate to 0.00385160663723082\n",
      "Epoch: 60 cost time: 0.07023286819458008\n",
      "Epoch: 60, Steps: 11 | Train Loss: 0.0066918  Test Loss: 0.0048489\n",
      "Validation loss decreased (0.006836 --> 0.006692).  Saving model ...\n",
      "Updating learning rate to 0.003739685241083054\n",
      "Epoch: 61 cost time: 0.07271504402160645\n",
      "Epoch: 61, Steps: 11 | Train Loss: 0.0073522  Test Loss: 0.0049417\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00362436598126825\n",
      "Epoch: 62 cost time: 0.07319259643554688\n",
      "Epoch: 62, Steps: 11 | Train Loss: 0.0065432  Test Loss: 0.0049592\n",
      "Validation loss decreased (0.006692 --> 0.006543).  Saving model ...\n",
      "Updating learning rate to 0.0035059649398618154\n",
      "Epoch: 63 cost time: 0.06770777702331543\n",
      "Epoch: 63, Steps: 11 | Train Loss: 0.0066847  Test Loss: 0.0055126\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0033848066458882892\n",
      "Epoch: 64 cost time: 0.07444953918457031\n",
      "Epoch: 64, Steps: 11 | Train Loss: 0.0073928  Test Loss: 0.0049798\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.003261223185809845\n",
      "Epoch: 65 cost time: 0.07255911827087402\n",
      "Epoch: 65, Steps: 11 | Train Loss: 0.0070477  Test Loss: 0.0052990\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.003135553293300376\n",
      "Epoch: 66 cost time: 0.06890106201171875\n",
      "Epoch: 66, Steps: 11 | Train Loss: 0.0070493  Test Loss: 0.0058899\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.003008141420800042\n",
      "Epoch: 67 cost time: 0.08059334754943848\n",
      "Epoch: 67, Steps: 11 | Train Loss: 0.0065581  Test Loss: 0.0058688\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.0028793367953950785\n",
      "Epoch: 68 cost time: 0.07017302513122559\n",
      "Epoch: 68, Steps: 11 | Train Loss: 0.0068798  Test Loss: 0.0049575\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.0027494924616106187\n",
      "Epoch: 69 cost time: 0.07165646553039551\n",
      "Epoch: 69, Steps: 11 | Train Loss: 0.0063953  Test Loss: 0.0045718\n",
      "Validation loss decreased (0.006543 --> 0.006395).  Saving model ...\n",
      "Updating learning rate to 0.002618964313740198\n",
      "Epoch: 70 cost time: 0.07517576217651367\n",
      "Epoch: 70, Steps: 11 | Train Loss: 0.0065903  Test Loss: 0.0047394\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.002488110120364228\n",
      "Epoch: 71 cost time: 0.08135247230529785\n",
      "Epoch: 71, Steps: 11 | Train Loss: 0.0062872  Test Loss: 0.0046243\n",
      "Validation loss decreased (0.006395 --> 0.006287).  Saving model ...\n",
      "Updating learning rate to 0.0023572885437311885\n",
      "Epoch: 72 cost time: 0.07302498817443848\n",
      "Epoch: 72, Steps: 11 | Train Loss: 0.0055940  Test Loss: 0.0049103\n",
      "Validation loss decreased (0.006287 --> 0.005594).  Saving model ...\n",
      "Updating learning rate to 0.002226858156689337\n",
      "Epoch: 73 cost time: 0.07921433448791504\n",
      "Epoch: 73, Steps: 11 | Train Loss: 0.0062404  Test Loss: 0.0046273\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.002097176459863446\n",
      "Epoch: 74 cost time: 0.06916570663452148\n",
      "Epoch: 74, Steps: 11 | Train Loss: 0.0061647  Test Loss: 0.0045910\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0019685989017704537\n",
      "Epoch: 75 cost time: 0.0666494369506836\n",
      "Epoch: 75, Steps: 11 | Train Loss: 0.0059070  Test Loss: 0.0048669\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.001841477904559777\n",
      "Epoch: 76 cost time: 0.06769704818725586\n",
      "Epoch: 76, Steps: 11 | Train Loss: 0.0061865  Test Loss: 0.0052302\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0017161618980486967\n",
      "Epoch: 77 cost time: 0.0684814453125\n",
      "Epoch: 77, Steps: 11 | Train Loss: 0.0064160  Test Loss: 0.0046646\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.0015929943647004514\n",
      "Epoch: 78 cost time: 0.06762528419494629\n",
      "Epoch: 78, Steps: 11 | Train Loss: 0.0059394  Test Loss: 0.0047423\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.0014723128981626776\n",
      "Epoch: 79 cost time: 0.07594561576843262\n",
      "Epoch: 79, Steps: 11 | Train Loss: 0.0056438  Test Loss: 0.0045924\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.0013544482779466913\n",
      "Epoch: 80 cost time: 0.07271504402160645\n",
      "Epoch: 80, Steps: 11 | Train Loss: 0.0055682  Test Loss: 0.0045597\n",
      "Validation loss decreased (0.005594 --> 0.005568).  Saving model ...\n",
      "Updating learning rate to 0.0012397235627838344\n",
      "Epoch: 81 cost time: 0.06811094284057617\n",
      "Epoch: 81, Steps: 11 | Train Loss: 0.0055599  Test Loss: 0.0045123\n",
      "Validation loss decreased (0.005568 --> 0.005560).  Saving model ...\n",
      "Updating learning rate to 0.0011284532051439504\n",
      "Epoch: 82 cost time: 0.0699300765991211\n",
      "Epoch: 82, Steps: 11 | Train Loss: 0.0057545  Test Loss: 0.0045640\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.001020942189343022\n",
      "Epoch: 83 cost time: 0.06731224060058594\n",
      "Epoch: 83, Steps: 11 | Train Loss: 0.0058012  Test Loss: 0.0045661\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0009174851956023624\n",
      "Epoch: 84 cost time: 0.06793403625488281\n",
      "Epoch: 84, Steps: 11 | Train Loss: 0.0056763  Test Loss: 0.0046655\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0008183657923506057\n",
      "Epoch: 85 cost time: 0.07477474212646484\n",
      "Epoch: 85, Steps: 11 | Train Loss: 0.0054725  Test Loss: 0.0045119\n",
      "Validation loss decreased (0.005560 --> 0.005473).  Saving model ...\n",
      "Updating learning rate to 0.000723855658982361\n",
      "Epoch: 86 cost time: 0.07110476493835449\n",
      "Epoch: 86, Steps: 11 | Train Loss: 0.0053806  Test Loss: 0.0044861\n",
      "Validation loss decreased (0.005473 --> 0.005381).  Saving model ...\n",
      "Updating learning rate to 0.0006342138412038708\n",
      "Epoch: 87 cost time: 0.07088112831115723\n",
      "Epoch: 87, Steps: 11 | Train Loss: 0.0050966  Test Loss: 0.0044783\n",
      "Validation loss decreased (0.005381 --> 0.005097).  Saving model ...\n",
      "Updating learning rate to 0.0005496860410067403\n",
      "Epoch: 88 cost time: 0.07603573799133301\n",
      "Epoch: 88, Steps: 11 | Train Loss: 0.0052210  Test Loss: 0.0044855\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00047050394321585307\n",
      "Epoch: 89 cost time: 0.06705689430236816\n",
      "Epoch: 89, Steps: 11 | Train Loss: 0.0049464  Test Loss: 0.0044509\n",
      "Validation loss decreased (0.005097 --> 0.004946).  Saving model ...\n",
      "Updating learning rate to 0.00039688458045737505\n",
      "Epoch: 90 cost time: 0.0661470890045166\n",
      "Epoch: 90, Steps: 11 | Train Loss: 0.0052005  Test Loss: 0.0044793\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0003290297382874084\n",
      "Epoch: 91 cost time: 0.07238554954528809\n",
      "Epoch: 91, Steps: 11 | Train Loss: 0.0052101  Test Loss: 0.0044684\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0002671254021118083\n",
      "Epoch: 92 cost time: 0.06670570373535156\n",
      "Epoch: 92, Steps: 11 | Train Loss: 0.0051992  Test Loss: 0.0044771\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0002113412474131142\n",
      "Epoch: 93 cost time: 0.07016611099243164\n",
      "Epoch: 93, Steps: 11 | Train Loss: 0.0050977  Test Loss: 0.0044715\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.00016183017468184583\n",
      "Epoch: 94 cost time: 0.08236932754516602\n",
      "Epoch: 94, Steps: 11 | Train Loss: 0.0051030  Test Loss: 0.0044765\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.00011872789032688815\n",
      "Epoch: 95 cost time: 0.06852388381958008\n",
      "Epoch: 95, Steps: 11 | Train Loss: 0.0050090  Test Loss: 0.0044711\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 8.21525347136544e-05\n",
      "Epoch: 96 cost time: 0.06803464889526367\n",
      "Epoch: 96, Steps: 11 | Train Loss: 0.0049809  Test Loss: 0.0044761\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 5.220435834955847e-05\n",
      "Epoch: 97 cost time: 0.07174944877624512\n",
      "Epoch: 97, Steps: 11 | Train Loss: 0.0050599  Test Loss: 0.0044670\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 2.89654471043424e-05\n",
      "Epoch: 98 cost time: 0.07872509956359863\n",
      "Epoch: 98, Steps: 11 | Train Loss: 0.0050053  Test Loss: 0.0044643\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 1.2499497218412443e-05\n",
      "Epoch: 99 cost time: 0.06836676597595215\n",
      "Epoch: 99, Steps: 11 | Train Loss: 0.0050735  Test Loss: 0.0044639\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 2.851640715871923e-06\n",
      "Epoch: 100 cost time: 0.07399439811706543\n",
      "Epoch: 100, Steps: 11 | Train Loss: 0.0050476  Test Loss: 0.0044639\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 4.8321700779777556e-08\n",
      ">>>>>>>testing : B0007_PathFormer_sl36_pl1_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 132\n",
      "Inference time:  0.0020000457763671873\n",
      "rmse:0.011055969633162022, mae:0.0044508930295705795, rse:0.08806111663579941\n",
      "Use CPU\n",
      ">>>>>>>start training : B0007_PathFormer_sl36_pl1_2>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train1 96\n",
      "train2 132\n",
      "train3 132\n",
      "test 132\n",
      "Epoch: 1 cost time: 0.0798346996307373\n",
      "Epoch: 1, Steps: 11 | Train Loss: 0.0732989  Test Loss: 0.0541788\n",
      "Validation loss decreased (inf --> 0.073299).  Saving model ...\n",
      "Updating learning rate to 0.00020743212569329043\n",
      "Epoch: 2 cost time: 0.07601141929626465\n",
      "Epoch: 2, Steps: 11 | Train Loss: 0.0688258  Test Loss: 0.0505718\n",
      "Validation loss decreased (0.073299 --> 0.068826).  Saving model ...\n",
      "Updating learning rate to 0.00022968247236289548\n",
      "Epoch: 3 cost time: 0.0828857421875\n",
      "Epoch: 3, Steps: 11 | Train Loss: 0.0643319  Test Loss: 0.0460841\n",
      "Validation loss decreased (0.068826 --> 0.064332).  Saving model ...\n",
      "Updating learning rate to 0.0002666132338645091\n",
      "Epoch: 4 cost time: 0.0779409408569336\n",
      "Epoch: 4, Steps: 11 | Train Loss: 0.0573036  Test Loss: 0.0393795\n",
      "Validation loss decreased (0.064332 --> 0.057304).  Saving model ...\n",
      "Updating learning rate to 0.0003179956818136062\n",
      "Epoch: 5 cost time: 0.09799695014953613\n",
      "Epoch: 5, Steps: 11 | Train Loss: 0.0467069  Test Loss: 0.0281551\n",
      "Validation loss decreased (0.057304 --> 0.046707).  Saving model ...\n",
      "Updating learning rate to 0.0003835115822005313\n",
      "Epoch: 6 cost time: 0.07826566696166992\n",
      "Epoch: 6, Steps: 11 | Train Loss: 0.0294204  Test Loss: 0.0111790\n",
      "Validation loss decreased (0.046707 --> 0.029420).  Saving model ...\n",
      "Updating learning rate to 0.0004627551663531305\n",
      "Epoch: 7 cost time: 0.0815892219543457\n",
      "Epoch: 7, Steps: 11 | Train Loss: 0.0239233  Test Loss: 0.0163603\n",
      "Validation loss decreased (0.029420 --> 0.023923).  Saving model ...\n",
      "Updating learning rate to 0.0005552356440398963\n",
      "Epoch: 8 cost time: 0.08044815063476562\n",
      "Epoch: 8, Steps: 11 | Train Loss: 0.0217047  Test Loss: 0.0117682\n",
      "Validation loss decreased (0.023923 --> 0.021705).  Saving model ...\n",
      "Updating learning rate to 0.0006603802431488772\n",
      "Epoch: 9 cost time: 0.0777883529663086\n",
      "Epoch: 9, Steps: 11 | Train Loss: 0.0209745  Test Loss: 0.0136333\n",
      "Validation loss decreased (0.021705 --> 0.020974).  Saving model ...\n",
      "Updating learning rate to 0.0007775377571162803\n",
      "Epoch: 10 cost time: 0.07669353485107422\n",
      "Epoch: 10, Steps: 11 | Train Loss: 0.0198676  Test Loss: 0.0105072\n",
      "Validation loss decreased (0.020974 --> 0.019868).  Saving model ...\n",
      "Updating learning rate to 0.0009059825781340073\n",
      "Epoch: 11 cost time: 0.08192324638366699\n",
      "Epoch: 11, Steps: 11 | Train Loss: 0.0185904  Test Loss: 0.0105085\n",
      "Validation loss decreased (0.019868 --> 0.018590).  Saving model ...\n",
      "Updating learning rate to 0.0010449191911566782\n",
      "Epoch: 12 cost time: 0.07780981063842773\n",
      "Epoch: 12, Steps: 11 | Train Loss: 0.0176745  Test Loss: 0.0098553\n",
      "Validation loss decreased (0.018590 --> 0.017674).  Saving model ...\n",
      "Updating learning rate to 0.001193487100874806\n",
      "Epoch: 13 cost time: 0.07659721374511719\n",
      "Epoch: 13, Steps: 11 | Train Loss: 0.0177845  Test Loss: 0.0107594\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0013507661611392116\n",
      "Epoch: 14 cost time: 0.08780360221862793\n",
      "Epoch: 14, Steps: 11 | Train Loss: 0.0171296  Test Loss: 0.0085396\n",
      "Validation loss decreased (0.017674 --> 0.017130).  Saving model ...\n",
      "Updating learning rate to 0.0015157822738292221\n",
      "Epoch: 15 cost time: 0.07573771476745605\n",
      "Epoch: 15, Steps: 11 | Train Loss: 0.0143913  Test Loss: 0.0074836\n",
      "Validation loss decreased (0.017130 --> 0.014391).  Saving model ...\n",
      "Updating learning rate to 0.0016875134218690617\n",
      "Epoch: 16 cost time: 0.07872200012207031\n",
      "Epoch: 16, Steps: 11 | Train Loss: 0.0130677  Test Loss: 0.0088077\n",
      "Validation loss decreased (0.014391 --> 0.013068).  Saving model ...\n",
      "Updating learning rate to 0.0018648959990273258\n",
      "Epoch: 17 cost time: 0.08564019203186035\n",
      "Epoch: 17, Steps: 11 | Train Loss: 0.0143634  Test Loss: 0.0068285\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0020468313972963155\n",
      "Epoch: 18 cost time: 0.07801580429077148\n",
      "Epoch: 18, Steps: 11 | Train Loss: 0.0127004  Test Loss: 0.0068744\n",
      "Validation loss decreased (0.013068 --> 0.012700).  Saving model ...\n",
      "Updating learning rate to 0.002232192811052702\n",
      "Epoch: 19 cost time: 0.07715916633605957\n",
      "Epoch: 19, Steps: 11 | Train Loss: 0.0110898  Test Loss: 0.0067986\n",
      "Validation loss decreased (0.012700 --> 0.011090).  Saving model ...\n",
      "Updating learning rate to 0.0024198322158583817\n",
      "Epoch: 20 cost time: 0.0829019546508789\n",
      "Epoch: 20, Steps: 11 | Train Loss: 0.0107829  Test Loss: 0.0067720\n",
      "Validation loss decreased (0.011090 --> 0.010783).  Saving model ...\n",
      "Updating learning rate to 0.0026085874786787467\n",
      "Epoch: 21 cost time: 0.07729363441467285\n",
      "Epoch: 21, Steps: 11 | Train Loss: 0.0106116  Test Loss: 0.0066320\n",
      "Validation loss decreased (0.010783 --> 0.010612).  Saving model ...\n",
      "Updating learning rate to 0.002797289555481671\n",
      "Epoch: 22 cost time: 0.07773661613464355\n",
      "Epoch: 22, Steps: 11 | Train Loss: 0.0094063  Test Loss: 0.0062873\n",
      "Validation loss decreased (0.010612 --> 0.009406).  Saving model ...\n",
      "Updating learning rate to 0.002984769731639334\n",
      "Epoch: 23 cost time: 0.07978081703186035\n",
      "Epoch: 23, Steps: 11 | Train Loss: 0.0105239  Test Loss: 0.0105152\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0031698668602898996\n",
      "Epoch: 24 cost time: 0.07759404182434082\n",
      "Epoch: 24, Steps: 11 | Train Loss: 0.0113529  Test Loss: 0.0071549\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.003351434553828701\n",
      "Epoch: 25 cost time: 0.08042740821838379\n",
      "Epoch: 25, Steps: 11 | Train Loss: 0.0114927  Test Loss: 0.0060460\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0035283482839888903\n",
      "Epoch: 26 cost time: 0.0796504020690918\n",
      "Epoch: 26, Steps: 11 | Train Loss: 0.0105982  Test Loss: 0.0092488\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.003699512346537615\n",
      "Epoch: 27 cost time: 0.07783794403076172\n",
      "Epoch: 27, Steps: 11 | Train Loss: 0.0114556  Test Loss: 0.0092606\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.003863866647452345\n",
      "Epoch: 28 cost time: 0.1512010097503662\n",
      "Epoch: 28, Steps: 11 | Train Loss: 0.0115325  Test Loss: 0.0066322\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.004020393268547556\n",
      "Epoch: 29 cost time: 0.20884227752685547\n",
      "Epoch: 29, Steps: 11 | Train Loss: 0.0095639  Test Loss: 0.0063157\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 0.004168122771887976\n",
      "Epoch: 30 cost time: 0.20459842681884766\n",
      "Epoch: 30, Steps: 11 | Train Loss: 0.0105258  Test Loss: 0.0055709\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 0.004306140203942409\n",
      "Epoch: 31 cost time: 0.20513343811035156\n",
      "Epoch: 31, Steps: 11 | Train Loss: 0.0098987  Test Loss: 0.0057365\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 0.004433590762291778\n",
      "Epoch: 32 cost time: 0.20804834365844727\n",
      "Epoch: 32, Steps: 11 | Train Loss: 0.0095364  Test Loss: 0.0071358\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 0.004549685089794972\n",
      "Epoch: 33 cost time: 0.20573139190673828\n",
      "Epoch: 33, Steps: 11 | Train Loss: 0.0096205  Test Loss: 0.0055782\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 0.004653704163423424\n",
      "Epoch: 34 cost time: 0.20341038703918457\n",
      "Epoch: 34, Steps: 11 | Train Loss: 0.0083044  Test Loss: 0.0056134\n",
      "Validation loss decreased (0.009406 --> 0.008304).  Saving model ...\n",
      "Updating learning rate to 0.004745003747485712\n",
      "Epoch: 35 cost time: 0.20278024673461914\n",
      "Epoch: 35, Steps: 11 | Train Loss: 0.0082578  Test Loss: 0.0071116\n",
      "Validation loss decreased (0.008304 --> 0.008258).  Saving model ...\n",
      "Updating learning rate to 0.0048230183836614204\n",
      "Epoch: 36 cost time: 0.20086383819580078\n",
      "Epoch: 36, Steps: 11 | Train Loss: 0.0088001  Test Loss: 0.0053925\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004887264893132239\n",
      "Epoch: 37 cost time: 0.20882678031921387\n",
      "Epoch: 37, Steps: 11 | Train Loss: 0.0082207  Test Loss: 0.0061195\n",
      "Validation loss decreased (0.008258 --> 0.008221).  Saving model ...\n",
      "Updating learning rate to 0.004937345369120049\n",
      "Epoch: 38 cost time: 0.202225923538208\n",
      "Epoch: 38, Steps: 11 | Train Loss: 0.0100365  Test Loss: 0.0069685\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004972949641297917\n",
      "Epoch: 39 cost time: 0.20087146759033203\n",
      "Epoch: 39, Steps: 11 | Train Loss: 0.0106682  Test Loss: 0.0057387\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004993857196810804\n",
      "Epoch: 40 cost time: 0.20407724380493164\n",
      "Epoch: 40, Steps: 11 | Train Loss: 0.0090284  Test Loss: 0.0066162\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.00499997167829922\n",
      "Epoch: 41 cost time: 0.20201992988586426\n",
      "Epoch: 41, Steps: 11 | Train Loss: 0.0085459  Test Loss: 0.0056611\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0049959227761243\n",
      "Epoch: 42 cost time: 0.19888615608215332\n",
      "Epoch: 42, Steps: 11 | Train Loss: 0.0081856  Test Loss: 0.0052362\n",
      "Validation loss decreased (0.008221 --> 0.008186).  Saving model ...\n",
      "Updating learning rate to 0.004985032750518793\n",
      "Epoch: 43 cost time: 0.201460599899292\n",
      "Epoch: 43, Steps: 11 | Train Loss: 0.0081441  Test Loss: 0.0058892\n",
      "Validation loss decreased (0.008186 --> 0.008144).  Saving model ...\n",
      "Updating learning rate to 0.004967331450285928\n",
      "Epoch: 44 cost time: 0.2027723789215088\n",
      "Epoch: 44, Steps: 11 | Train Loss: 0.0080903  Test Loss: 0.0051271\n",
      "Validation loss decreased (0.008144 --> 0.008090).  Saving model ...\n",
      "Updating learning rate to 0.004942867393459239\n",
      "Epoch: 45 cost time: 0.20170164108276367\n",
      "Epoch: 45, Steps: 11 | Train Loss: 0.0075909  Test Loss: 0.0060881\n",
      "Validation loss decreased (0.008090 --> 0.007591).  Saving model ...\n",
      "Updating learning rate to 0.004911707634318015\n",
      "Epoch: 46 cost time: 0.20401787757873535\n",
      "Epoch: 46, Steps: 11 | Train Loss: 0.0075746  Test Loss: 0.0054676\n",
      "Validation loss decreased (0.007591 --> 0.007575).  Saving model ...\n",
      "Updating learning rate to 0.004873937579596172\n",
      "Epoch: 47 cost time: 0.20370221138000488\n",
      "Epoch: 47, Steps: 11 | Train Loss: 0.0075458  Test Loss: 0.0056772\n",
      "Validation loss decreased (0.007575 --> 0.007546).  Saving model ...\n",
      "Updating learning rate to 0.004829660754388339\n",
      "Epoch: 48 cost time: 0.20293927192687988\n",
      "Epoch: 48, Steps: 11 | Train Loss: 0.0073940  Test Loss: 0.0056245\n",
      "Validation loss decreased (0.007546 --> 0.007394).  Saving model ...\n",
      "Updating learning rate to 0.004778998518394766\n",
      "Epoch: 49 cost time: 0.203261137008667\n",
      "Epoch: 49, Steps: 11 | Train Loss: 0.0075413  Test Loss: 0.0053994\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.004722089733282822\n",
      "Epoch: 50 cost time: 0.20508337020874023\n",
      "Epoch: 50, Steps: 11 | Train Loss: 0.0083073  Test Loss: 0.0061184\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004659090382076819\n",
      "Epoch: 51 cost time: 0.20373749732971191\n",
      "Epoch: 51, Steps: 11 | Train Loss: 0.0078247  Test Loss: 0.0053684\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004590173141619381\n",
      "Epoch: 52 cost time: 0.20290255546569824\n",
      "Epoch: 52, Steps: 11 | Train Loss: 0.0069677  Test Loss: 0.0055419\n",
      "Validation loss decreased (0.007394 --> 0.006968).  Saving model ...\n",
      "Updating learning rate to 0.004515526909276222\n",
      "Epoch: 53 cost time: 0.20213723182678223\n",
      "Epoch: 53, Steps: 11 | Train Loss: 0.0073613  Test Loss: 0.0059656\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0044353562851816\n",
      "Epoch: 54 cost time: 0.2030017375946045\n",
      "Epoch: 54, Steps: 11 | Train Loss: 0.0082441  Test Loss: 0.0055041\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.004349881011443566\n",
      "Epoch: 55 cost time: 0.2046372890472412\n",
      "Epoch: 55, Steps: 11 | Train Loss: 0.0071271  Test Loss: 0.0048899\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.004259335369846122\n",
      "Epoch: 56 cost time: 0.20247864723205566\n",
      "Epoch: 56, Steps: 11 | Train Loss: 0.0080552  Test Loss: 0.0054493\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.004163967539699138\n",
      "Epoch: 57 cost time: 0.20501303672790527\n",
      "Epoch: 57, Steps: 11 | Train Loss: 0.0074857  Test Loss: 0.0050912\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.004064038917596108\n",
      "Epoch: 58 cost time: 0.20449566841125488\n",
      "Epoch: 58, Steps: 11 | Train Loss: 0.0066974  Test Loss: 0.0048958\n",
      "Validation loss decreased (0.006968 --> 0.006697).  Saving model ...\n",
      "Updating learning rate to 0.003959823400944265\n",
      "Epoch: 59 cost time: 0.20578384399414062\n",
      "Epoch: 59, Steps: 11 | Train Loss: 0.0082543  Test Loss: 0.0062266\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00385160663723082\n",
      "Epoch: 60 cost time: 0.2068328857421875\n",
      "Epoch: 60, Steps: 11 | Train Loss: 0.0082314  Test Loss: 0.0053508\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.003739685241083054\n",
      "Epoch: 61 cost time: 0.21035289764404297\n",
      "Epoch: 61, Steps: 11 | Train Loss: 0.0067691  Test Loss: 0.0047252\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.00362436598126825\n",
      "Epoch: 62 cost time: 0.2080669403076172\n",
      "Epoch: 62, Steps: 11 | Train Loss: 0.0068220  Test Loss: 0.0049140\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0035059649398618154\n",
      "Epoch: 63 cost time: 0.22710824012756348\n",
      "Epoch: 63, Steps: 11 | Train Loss: 0.0071339  Test Loss: 0.0056413\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.0033848066458882892\n",
      "Epoch: 64 cost time: 0.20900297164916992\n",
      "Epoch: 64, Steps: 11 | Train Loss: 0.0063567  Test Loss: 0.0049198\n",
      "Validation loss decreased (0.006697 --> 0.006357).  Saving model ...\n",
      "Updating learning rate to 0.003261223185809845\n",
      "Epoch: 65 cost time: 0.20475125312805176\n",
      "Epoch: 65, Steps: 11 | Train Loss: 0.0061248  Test Loss: 0.0048517\n",
      "Validation loss decreased (0.006357 --> 0.006125).  Saving model ...\n",
      "Updating learning rate to 0.003135553293300376\n",
      "Epoch: 66 cost time: 0.20476174354553223\n",
      "Epoch: 66, Steps: 11 | Train Loss: 0.0059891  Test Loss: 0.0053165\n",
      "Validation loss decreased (0.006125 --> 0.005989).  Saving model ...\n",
      "Updating learning rate to 0.003008141420800042\n",
      "Epoch: 67 cost time: 0.2037827968597412\n",
      "Epoch: 67, Steps: 11 | Train Loss: 0.0065804  Test Loss: 0.0057877\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0028793367953950785\n",
      "Epoch: 68 cost time: 0.20688343048095703\n",
      "Epoch: 68, Steps: 11 | Train Loss: 0.0070289  Test Loss: 0.0059679\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0027494924616106187\n",
      "Epoch: 69 cost time: 0.20338773727416992\n",
      "Epoch: 69, Steps: 11 | Train Loss: 0.0065294  Test Loss: 0.0050576\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.002618964313740198\n",
      "Epoch: 70 cost time: 0.20427894592285156\n",
      "Epoch: 70, Steps: 11 | Train Loss: 0.0060494  Test Loss: 0.0046851\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.002488110120364228\n",
      "Epoch: 71 cost time: 0.20448827743530273\n",
      "Epoch: 71, Steps: 11 | Train Loss: 0.0057306  Test Loss: 0.0049882\n",
      "Validation loss decreased (0.005989 --> 0.005731).  Saving model ...\n",
      "Updating learning rate to 0.0023572885437311885\n",
      "Epoch: 72 cost time: 0.2044963836669922\n",
      "Epoch: 72, Steps: 11 | Train Loss: 0.0055272  Test Loss: 0.0048076\n",
      "Validation loss decreased (0.005731 --> 0.005527).  Saving model ...\n",
      "Updating learning rate to 0.002226858156689337\n",
      "Epoch: 73 cost time: 0.20522093772888184\n",
      "Epoch: 73, Steps: 11 | Train Loss: 0.0053645  Test Loss: 0.0048372\n",
      "Validation loss decreased (0.005527 --> 0.005365).  Saving model ...\n",
      "Updating learning rate to 0.002097176459863446\n",
      "Epoch: 74 cost time: 0.20497822761535645\n",
      "Epoch: 74, Steps: 11 | Train Loss: 0.0058489  Test Loss: 0.0046953\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0019685989017704537\n",
      "Epoch: 75 cost time: 0.2045304775238037\n",
      "Epoch: 75, Steps: 11 | Train Loss: 0.0057495  Test Loss: 0.0050413\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.001841477904559777\n",
      "Epoch: 76 cost time: 0.20377039909362793\n",
      "Epoch: 76, Steps: 11 | Train Loss: 0.0058078  Test Loss: 0.0048856\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.0017161618980486967\n",
      "Epoch: 77 cost time: 0.20438170433044434\n",
      "Epoch: 77, Steps: 11 | Train Loss: 0.0053699  Test Loss: 0.0046917\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0015929943647004514\n",
      "Epoch: 78 cost time: 0.20495247840881348\n",
      "Epoch: 78, Steps: 11 | Train Loss: 0.0054696  Test Loss: 0.0049436\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 0.0014723128981626776\n",
      "Epoch: 79 cost time: 0.204697847366333\n",
      "Epoch: 79, Steps: 11 | Train Loss: 0.0053770  Test Loss: 0.0047439\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 0.0013544482779466913\n",
      "Epoch: 80 cost time: 0.20620107650756836\n",
      "Epoch: 80, Steps: 11 | Train Loss: 0.0051210  Test Loss: 0.0046761\n",
      "Validation loss decreased (0.005365 --> 0.005121).  Saving model ...\n",
      "Updating learning rate to 0.0012397235627838344\n",
      "Epoch: 81 cost time: 0.2036442756652832\n",
      "Epoch: 81, Steps: 11 | Train Loss: 0.0051196  Test Loss: 0.0046331\n",
      "Validation loss decreased (0.005121 --> 0.005120).  Saving model ...\n",
      "Updating learning rate to 0.0011284532051439504\n",
      "Epoch: 82 cost time: 0.20926952362060547\n",
      "Epoch: 82, Steps: 11 | Train Loss: 0.0050186  Test Loss: 0.0045858\n",
      "Validation loss decreased (0.005120 --> 0.005019).  Saving model ...\n",
      "Updating learning rate to 0.001020942189343022\n",
      "Epoch: 83 cost time: 0.21081137657165527\n",
      "Epoch: 83, Steps: 11 | Train Loss: 0.0050316  Test Loss: 0.0046934\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0009174851956023624\n",
      "Epoch: 84 cost time: 0.20904302597045898\n",
      "Epoch: 84, Steps: 11 | Train Loss: 0.0051299  Test Loss: 0.0046203\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0008183657923506057\n",
      "Epoch: 85 cost time: 0.2093513011932373\n",
      "Epoch: 85, Steps: 11 | Train Loss: 0.0052704  Test Loss: 0.0045604\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 0.000723855658982361\n",
      "Epoch: 86 cost time: 0.20591044425964355\n",
      "Epoch: 86, Steps: 11 | Train Loss: 0.0052296  Test Loss: 0.0047091\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 0.0006342138412038708\n",
      "Epoch: 87 cost time: 0.20685744285583496\n",
      "Epoch: 87, Steps: 11 | Train Loss: 0.0049090  Test Loss: 0.0045965\n",
      "Validation loss decreased (0.005019 --> 0.004909).  Saving model ...\n",
      "Updating learning rate to 0.0005496860410067403\n",
      "Epoch: 88 cost time: 0.20951294898986816\n",
      "Epoch: 88, Steps: 11 | Train Loss: 0.0048286  Test Loss: 0.0045768\n",
      "Validation loss decreased (0.004909 --> 0.004829).  Saving model ...\n",
      "Updating learning rate to 0.00047050394321585307\n",
      "Epoch: 89 cost time: 0.20888471603393555\n",
      "Epoch: 89, Steps: 11 | Train Loss: 0.0047759  Test Loss: 0.0045694\n",
      "Validation loss decreased (0.004829 --> 0.004776).  Saving model ...\n",
      "Updating learning rate to 0.00039688458045737505\n",
      "Epoch: 90 cost time: 0.20974040031433105\n",
      "Epoch: 90, Steps: 11 | Train Loss: 0.0045898  Test Loss: 0.0046366\n",
      "Validation loss decreased (0.004776 --> 0.004590).  Saving model ...\n",
      "Updating learning rate to 0.0003290297382874084\n",
      "Epoch: 91 cost time: 0.21094703674316406\n",
      "Epoch: 91, Steps: 11 | Train Loss: 0.0047215  Test Loss: 0.0045586\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0002671254021118083\n",
      "Epoch: 92 cost time: 0.20968866348266602\n",
      "Epoch: 92, Steps: 11 | Train Loss: 0.0047382  Test Loss: 0.0045585\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 0.0002113412474131142\n",
      "Epoch: 93 cost time: 0.20811939239501953\n",
      "Epoch: 93, Steps: 11 | Train Loss: 0.0044234  Test Loss: 0.0045748\n",
      "Validation loss decreased (0.004590 --> 0.004423).  Saving model ...\n",
      "Updating learning rate to 0.00016183017468184583\n",
      "Epoch: 94 cost time: 0.20784974098205566\n",
      "Epoch: 94, Steps: 11 | Train Loss: 0.0046673  Test Loss: 0.0045608\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.00011872789032688815\n",
      "Epoch: 95 cost time: 0.20904755592346191\n",
      "Epoch: 95, Steps: 11 | Train Loss: 0.0046421  Test Loss: 0.0045563\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 8.21525347136544e-05\n",
      "Epoch: 96 cost time: 0.21058988571166992\n",
      "Epoch: 96, Steps: 11 | Train Loss: 0.0045622  Test Loss: 0.0045501\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 5.220435834955847e-05\n",
      "Epoch: 97 cost time: 0.20838236808776855\n",
      "Epoch: 97, Steps: 11 | Train Loss: 0.0044772  Test Loss: 0.0045637\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 2.89654471043424e-05\n",
      "Epoch: 98 cost time: 0.20600366592407227\n",
      "Epoch: 98, Steps: 11 | Train Loss: 0.0043508  Test Loss: 0.0045635\n",
      "Validation loss decreased (0.004423 --> 0.004351).  Saving model ...\n",
      "Updating learning rate to 1.2499497218412443e-05\n",
      "Epoch: 99 cost time: 0.2080526351928711\n",
      "Epoch: 99, Steps: 11 | Train Loss: 0.0044948  Test Loss: 0.0045595\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 2.851640715871923e-06\n",
      "Epoch: 100 cost time: 0.20966577529907227\n",
      "Epoch: 100, Steps: 11 | Train Loss: 0.0043184  Test Loss: 0.0045584\n",
      "Validation loss decreased (0.004351 --> 0.004318).  Saving model ...\n",
      "Updating learning rate to 4.8321700779777556e-08\n",
      ">>>>>>>testing : B0007_PathFormer_sl36_pl1_2<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 132\n",
      "Inference time:  0.005497932434082031\n",
      "rmse:0.011124289594590664, mae:0.004558389075100422, rse:0.08860529214143753\n"
     ]
    }
   ],
   "source": [
    "fix_seed = 1024  # 随机数种子\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "class Args:\n",
    "    is_training = 1\n",
    "    model = 'PathFormer' \n",
    "    data = 'NASA'\n",
    "    root_path = './NASA'\n",
    "    checkpoints = './checkpoints/'\n",
    "    seq_len = 36\n",
    "    pred_len = 1\n",
    "    d_ff = 64\n",
    "    num_nodes = 1\n",
    "    layer_nums = 2\n",
    "    k = 1\n",
    "    num_experts_list = [4, 4]\n",
    "    patch_size_list = [6, 9, 12, 18, 2, 3, 4, 6]  \n",
    "    revin = 1\n",
    "    drop = 0.1\n",
    "    metric = 'mae'\n",
    "    num_workers = 0\n",
    "    itr = 3\n",
    "    train_epochs = 100\n",
    "    batch_size = 32\n",
    "    patience = 100\n",
    "    learning_rate = 0.005\n",
    "    pct_start = 0.4\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '2'\n",
    "    threshold = 1.4\n",
    "    start_point = 20\n",
    "    d_emb = 64\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path4 = {'train1': 'B0006.csv', 'train2': 'B0007.csv', 'train3': 'B0005.csv', 'test': 'B0018.csv'}\n",
    "    data_path3 = {'train1': 'B0018.csv', 'train2': 'B0005.csv', 'train3': 'B0006.csv', 'test': 'B0007.csv'}\n",
    "    data_path2 = {'train1': 'B0007.csv', 'train2': 'B0005.csv', 'train3': 'B0018.csv', 'test': 'B0006.csv'}\n",
    "    data_path1 = {'train1': 'B0007.csv', 'train2': 'B0018.csv', 'train3': 'B0006.csv', 'test': 'B0005.csv'}\n",
    "\n",
    "    args = Args()\n",
    "\n",
    "    # 检查是否使用 GPU\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    args.patch_size_list = np.array(args.patch_size_list).reshape(args.layer_nums, -1).tolist()  # 将patch_size_list转换为列表，得到3*4的列表\n",
    "\n",
    "    for i in range(1): \n",
    "        if i == 0:\n",
    "            args.k = 3\n",
    "            args.is_training = 1\n",
    "            args.model = 'PathFormer'\n",
    "            args.data_path = data_path3\n",
    "            args.train_epochs = 100\n",
    "            args.seq_len = 36\n",
    "            args.start_point = 66\n",
    "            args.d_emb = 36\n",
    "            args.learning_rate = 0.005\n",
    "            args.revin = 1\n",
    "        args.model_id = args.data_path['test'].split('.')[0]\n",
    "        train_test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
